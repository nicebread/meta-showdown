---
title: "Supplementary Appendix B: 10,000 vs. 1,000 replications"
date: "`r Sys.Date()`"
output: pdf_document
classoption: landscape
---


```{r setup, include=FALSE}
library(dplyr)
library(knitr)
library(ggplot2)
library(reshape2)

# library(rmarkdown);render('Appendix_B-10000_vs _1000_sims.Rmd', output_format='pdf_document', clean=FALSE, run_pandoc = FALSE)
# library(rmarkdown);render('Appendix_B-10000_vs _1000_sims.Rmd', output_format='pdf_document')

```


```{r echo=FALSE, cache=FALSE, results="hide"}
## ======================================================================
## This file loads the results of the meta-analyses (which were generated in 2-analysisFramework.R)
## and computes summaries of them (such as mean error, MSE, coverage, etc.)
## ======================================================================

# load the results files which were generated in 2-analysisFramework.R,
# combine them into one large data frame
analysisFiles <- list.files("analysisParts", pattern=".*\\.RData", full.names=TRUE)


# loop through all files
res_list <- list()
for (f in analysisFiles) {
  print(f)
  load(f)	# the simulation data frame always is called "res"
  
  # make pcurve.evidence and pcurve.lack two separate methods
  res$method <- as.character(res$method)
  res$method[res$method=="pcurve"] <- paste0("pcurve.", res$term[res$method=="pcurve"])
  
  # Slice into 10 batches of 1000 simulations
  res$simBatch <- factor(as.numeric(cut(res$id, 10)))
  
  res_list[[f]] <- res
}
res.final <- bind_rows(res_list)
rm(res_list)
str(res.final)


# Show conditions
#tab <- res.final %>% group_by(k, delta, censor, tau) %>% summarise(n.MA=length(unique(id)))
#print(tab, n=50)


# Reshape to wide format
#res.wide <- res.final %>% filter(term %in% c("b0", "kSig")) %>% dcast(id + condition + simBatch + k + delta + qrpEnv + censor + tau + method + term ~ variable, value.var="value")
res.wide <- res.final %>% 
	filter(term %in% c("b0"), method %in% c("reMA", "TF", "WAAP-WLS", "pcurve", "puniform", "PETPEESE", "3PSM", "4PSM")) %>% 
	dcast(id + condition + simBatch + k + delta + qrpEnv + censor + tau + method + term ~ variable, value.var="value")

# get id's with kSig >= 4
# incl <- res.wide %>% filter(term=="kSig") %>% group_by(id) %>% summarise(kSig_include = estimate >= 4)
# incl.ids <- incl$id[incl$kSig_include == TRUE]
#
# # Set p-curve and p-uniform to NA where kSig < 4
# res.wide$estimate[!res.wide$id %in% incl.ids & res.wide$method %in% c("puniform", "pcurve.b0") & res.wide$term == "b0"] <- NA


# define some meaningful labels for the plots
res.wide$delta.label <- factor(res.wide$delta, levels=unique(res.wide$delta), labels=paste0("delta = ", unique(res.wide$delta)))
res.wide$k.label <- factor(res.wide$k, levels=sort(unique(res.wide$k)), labels=paste0("k = ", sort(unique(res.wide$k))))
res.wide$censor.label <- factor(res.wide$censor, levels=unique(res.wide$censor), labels=paste0("Publication bias = ", unique(res.wide$censor)))
res.wide$tau.label <- factor(res.wide$tau, levels=unique(res.wide$tau), labels=paste0("tau = ", unique(res.wide$tau)))
res.wide$qrp.label <- factor(res.wide$qrpEnv, levels=unique(res.wide$qrpEnv), labels=paste0("QRP = ", unique(res.wide$qrpEnv)))

summ <- res.wide %>% group_by(condition, simBatch, k.label, delta, delta.label, censor.label, tau.label, qrp.label, method) %>% dplyr::summarise(
  meanEst		= mean(estimate, na.rm=TRUE),
  ME 			= mean(estimate - delta, na.rm=TRUE),
  RMSE			= sqrt(mean((estimate - delta)^2, na.rm=TRUE)),
  n.simulations = n()
)

# compute Monte Carlo Error (see Koehler, E., Brown, E., & Haneuse, S. J. P. A. (2009). On the Assessment of Monte Carlo Error in Simulation-Based Statistical Analyses. American Statistician, 63(2), 155–162. http://doi.org/10.1198/tast.2009.0030)
# MCE = SD of the Monte Carlo estimator, taken across repetitions of the simulation, whereas each simulation is based on the same design

MCE <- summ %>% group_by(k.label, delta, delta.label, censor.label, tau.label, qrp.label, method) %>% 
	dplyr::summarise(
		meanEst.MCE = sd(meanEst),
		ME.MCE = sd(ME),
		RMSE.MCE = sd(RMSE),
		n.batches = n()
	)
	
summary(MCE)
quantile(MCE$meanEst.MCE, probs=c(0.95))
quantile(MCE$RMSE.MCE, probs=c(0.95))

```

This is an online Appendix for:

Carter, E. C., Schönbrodt, F. D., Hilgard, J., &amp; Gervais, W. (2018). *Correcting for bias in psychology: A comparison of meta-analytic methods.* Retrieved from [https://osf.io/rf3ys/](https://osf.io/rf3ys/).


## Method ##

Simulations were run with 10,000 replications for a random selection of conditions. Then, these 10,000 runs were divided in 10 batches of 1000 simulations each. The following plots show points for each of the 10 batches. One can see that they are virtually identical, as the points of the different batches overlap.

## Variance in ES estimates between simulation batches

```{r MCE_ES, echo=FALSE, fig.width=11, fig.height=8, warning=FALSE}
ggplot(summ, aes(x=method, y=meanEst, color=simBatch, shape=factor(delta.label))) + geom_point() + geom_hline(aes(yintercept=delta), linetype="dotted") + facet_grid(k.label~tau.label~censor.label~QRP.) + coord_flip(ylim=c(-0.5, 1))
```


## Variance in RMSE between simulation batches

```{r MCE_RMSE, echo=FALSE, fig.width=11, fig.height=8, warning=FALSE}
ggplot(summ, aes(x=method, y=RMSE, color=simBatch, shape=delta.label)) + geom_point() + facet_grid(k.label~tau.label~censor.label) + coord_flip(ylim=c(-.8, .8))
```
